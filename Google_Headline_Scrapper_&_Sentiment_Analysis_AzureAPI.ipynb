{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCNRpr-Cn-zE",
        "outputId": "bf0eeae0-def6-47c3-c896-2d75a0d33627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive\n",
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/\n",
            "/content/drive/My Drive\n",
            "'16012023 SA8 056.pdf'                         'Investor Information Form.gdoc'\n",
            "'AlexOJ consent form (1).pdf'                  'Isreal Osaretin CV 2.docx'\n",
            "'AlexOJ consent form (2).pdf'                  'Mandate SW.docx'\n",
            "'AlexOJ consent form (3).pdf'                  'New Start Payroll Form (PAYE) (002).docx'\n",
            "'AlexOJ consent form.pdf'                      'SMART INDUCTION PACK JAN 2021 (WFH) (1).pdf'\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/                             'Starter-checklist_English_2021_2022 (1).pdf'\n",
            " CV.docx                                        Starter-checklist_English_2021_2022.gdoc\n",
            "'Fee Assessment Questionnaire Version 2.docx'   Starter-checklist_English_2021_2022.pdf\n",
            "'Fee Assessment Questionnaire Version 2.gdoc'  'Untitled document.gdoc'\n",
            "'Fee Status Questionaire.gdoc'                 'Untitled form.gform'\n",
            "'Fee Status Questionaire.pdf'                  'unturned audio'\n",
            "'How to get started with Drive.pdf'\n",
            "/content/drive/My Drive/Colab Notebooks\n",
            "'Google_Headline_Scrapper_&_Sentiment_Analysis.ipynb'   Untitled0.ipynb\n",
            " google_news_headlines_by_date.csv\n"
          ]
        }
      ],
      "source": [
        "# Code to mount google drive and change working directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%ls\n",
        "%cd drive/\n",
        "%ls\n",
        "%cd My\\ Drive\n",
        "%ls\n",
        "%cd Colab\\ Notebooks\n",
        "%ls\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MiF01EFLx_B"
      },
      "outputs": [],
      "source": [
        "### Google News Headline Scrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAew0dRLLgGj",
        "outputId": "d3a0ee4b-f7b2-40df-ff6a-75f193c2fc74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=a64c9e7adef149210be24074360ce1738e12c19594e79ca6f07e03059f87dd88\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics)\n",
            "  Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-textanalytics)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (4.10.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2024.2.2)\n",
            "Installing collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.30.1 isodate-0.6.1\n",
            "         date                                         headline_1  \\\n",
            "0  2024-04-03  S&P 500 Snaps Two-Day Losing Streak - WSJ - Th...   \n",
            "1  2024-04-02  S&P 500 Gains and Losses Today: Health Care St...   \n",
            "2  2024-04-01  S&P 500 Gains and Losses Today: Shares of Memo...   \n",
            "3  2024-03-31  The Magnificent 7 are no longer the only stock...   \n",
            "4  2024-03-28  The S&P 500 just turned in its best first quar...   \n",
            "\n",
            "                                          headline_2  \\\n",
            "0  S&P 500 Gains and Losses Today: Cosmetics Stoc...   \n",
            "1  Goldman Sachs says equal-weight S&P 500 is now...   \n",
            "2  S&P 500 Drops to Start New Quarter - WSJ - The...   \n",
            "3  The Magnificent 7 are no longer the only stock...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_3  \\\n",
            "0  Stock Market News From April 2, 2024: Dow, S&P...   \n",
            "1  S&P 500 trackers hit a record 27% of 2023 equi...   \n",
            "2  Micron Is Best-Performing Stock in S&P 500 on ...   \n",
            "3  12 Best S&P 500 Stocks To Buy According to Ana...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_4  \\\n",
            "0  S&P 500, Nasdaq close slightly higher after so...   \n",
            "1  S&P 500 chart analysis shows pullback is likel...   \n",
            "2  Snub VIX's Climb, Focus on S&P 500's Striking ...   \n",
            "3  The S&P 500 Just Notched a 5-Month Win Streak....   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_5  \\\n",
            "0  GE, 3M Spinoffs Join S&P 500, Divide Investors...   \n",
            "1  Should You Buy the 3 Highest-Paying Dividend S...   \n",
            "2  Stock Market News, April 1, 2024: S&P 500 Retr...   \n",
            "3  The S&P 500 Nets First Best Quarter In Five Ye...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_6  \\\n",
            "0  S&P 500 (SPX) today: Arista Networks Inc is a ...   \n",
            "1  S&P 500: 2 Stocks Turn $10,000 Into $39,643 In...   \n",
            "2  Stock Market News for Monday, April 1, 2024: D...   \n",
            "3  Is It Safe to Buy Stocks With the S&P 500 at a...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_7  \\\n",
            "0  Stock Market News From April 3, 2024: Dow Fall...   \n",
            "1  Bitcoin Follows the S&P 500 Lower. Watch This ...   \n",
            "2  The average S&P 500 stock is as overvalued as ...   \n",
            "3  Here's Why Vanguard's Largest Growth Fund Keep...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_8  \\\n",
            "0  Stock Market News, April 3, 2024: S&P 500 Clos...   \n",
            "1  S&P 500 Futures Drop in Premarket Trading; PVH...   \n",
            "2  Gold Hits an All-Time High. It’s Beating the S...   \n",
            "3  1 Stock That Could Outperform the S&P 500 With...   \n",
            "4                                                      \n",
            "\n",
            "                                          headline_9  \\\n",
            "0  Ulta Stock Is the Worst Performer in the S&P 5...   \n",
            "1                                                      \n",
            "2  Watch 'TopGun' ETF Outperforms S&P 500 - Bloom...   \n",
            "3                                                      \n",
            "4                                                      \n",
            "\n",
            "                                         headline_10  \\\n",
            "0  These 3 REITs Have Delivered Better Returns Th...   \n",
            "1                                                      \n",
            "2  Zacks Investment Ideas feature highlights: Rus...   \n",
            "3                                                      \n",
            "4                                                      \n",
            "\n",
            "                                         headline_11  \\\n",
            "0  Stock Market Today: Dow Jones ends with third ...   \n",
            "1                                                      \n",
            "2  Invest in Artificial Intelligence (AI) Stocks ...   \n",
            "3                                                      \n",
            "4                                                      \n",
            "\n",
            "                                         headline_12  \\\n",
            "0  Crypto staking rewards are now 450% higher tha...   \n",
            "1                                                      \n",
            "2  Nasdaq Index, Dow Jones, S&P 500 News: Bullish...   \n",
            "3                                                      \n",
            "4                                                      \n",
            "\n",
            "                                         headline_13  \n",
            "0  S&P 500 and DJIA enter 'Worst Months' of the y...  \n",
            "1                                                     \n",
            "2                                                     \n",
            "3                                                     \n",
            "4                                                     \n",
            "Filtered and saved articles to google_news_headlines_by_date.csv.\n"
          ]
        }
      ],
      "source": [
        "!pip install feedparser\n",
        "!pip install azure-ai-textanalytics\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import feedparser\n",
        "\n",
        "# Define your start and end dates for filtering\n",
        "start_date = datetime(2024, 3, 28)\n",
        "end_date = datetime(2024, 4, 4)\n",
        "\n",
        "rss_url = \"https://news.google.com/rss/search?q=%22S%26P+500%22+OR+%22SPX%22+OR+%22$SPY%22&hl=en-US&gl=US&ceid=US:en\"\n",
        "feed = feedparser.parse(rss_url)\n",
        "\n",
        "articles_by_date = {}\n",
        "\n",
        "for entry in feed.entries:\n",
        "    published_date = datetime(*entry.published_parsed[:6])\n",
        "\n",
        "    # Filter articles by the specified start and end dates\n",
        "    if start_date <= published_date <= end_date:\n",
        "        date_str = published_date.strftime('%Y-%m-%d')\n",
        "        headline = entry.title\n",
        "\n",
        "        if date_str in articles_by_date:\n",
        "            articles_by_date[date_str].append(headline)\n",
        "        else:\n",
        "            articles_by_date[date_str] = [headline]\n",
        "\n",
        "data_for_df = []\n",
        "for date, headlines in articles_by_date.items():\n",
        "    row = {'date': date}\n",
        "    for i, headline in enumerate(headlines):\n",
        "        row[f'headline_{i+1}'] = headline\n",
        "    data_for_df.append(row)\n",
        "\n",
        "df_articles = pd.DataFrame(data_for_df)\n",
        "df_articles = df_articles.fillna('')\n",
        "\n",
        "print(df_articles.head())\n",
        "\n",
        "output_file_name = 'google_news_headlines_by_date.csv'\n",
        "df_articles.to_csv(output_file_name, index=False)\n",
        "\n",
        "print(f\"Filtered and saved articles to {output_file_name}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2The-sWbLuPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d7a536-cb92-477a-f4a3-6c8501a4e4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-ai-textanalytics in /usr/local/lib/python3.10/dist-packages (5.3.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (1.30.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (1.1.28)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (4.10.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2024.2.2)\n",
            "Document: S&P 500 Snaps Two-Day Losing Streak - WSJ - The Wall Street Journal\n",
            "Sentiment: neutral, Scores: {'positive': 0.02, 'neutral': 0.77, 'negative': 0.21}\n",
            "Document: S&P 500 Gains and Losses Today: Health Care Stocks Move Lower - Investopedia\n",
            "Sentiment: neutral, Scores: {'positive': 0.02, 'neutral': 0.85, 'negative': 0.13}\n",
            "Document: S&P 500 Gains and Losses Today: Shares of Memory Chipmakers Push Higher - Investopedia\n",
            "Sentiment: neutral, Scores: {'positive': 0.04, 'neutral': 0.9, 'negative': 0.07}\n",
            "Document: The Magnificent 7 are no longer the only stocks driving S&P 500 to record highs - MarketWatch\n",
            "Sentiment: neutral, Scores: {'positive': 0.08, 'neutral': 0.83, 'negative': 0.1}\n",
            "Document: The S&P 500 just turned in its best first quarter since 2019 - CNN\n",
            "Sentiment: neutral, Scores: {'positive': 0.39, 'neutral': 0.59, 'negative': 0.02}\n",
            "Document: Beware Nvidia and the S&P 500 ‘index waltz,’ market-beating fund manager says - MarketWatch\n",
            "Sentiment: neutral, Scores: {'positive': 0.07, 'neutral': 0.81, 'negative': 0.11}\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-ai-textanalytics\n",
        "\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "import pandas as pd\n",
        "\n",
        "# Set up the Azure Text Analytics client\n",
        "def authenticate_client():\n",
        "    endpoint = \"https://sentimentanalysishp.cognitiveservices.azure.com/\"  # Replace with your endpoint\n",
        "    key = \"dedffb82877445dd93d95092e65b54bc\"  # Replace with your key\n",
        "    return TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
        "\n",
        "# Analyze sentiment of documents\n",
        "def analyze_sentiment(documents):\n",
        "    client = authenticate_client()\n",
        "    response = client.analyze_sentiment(documents=documents, language=\"en\")\n",
        "    return response\n",
        "\n",
        "# Adjust the CSV path to match your Google Drive structure\n",
        "csv_path = '/content/drive/My Drive/Colab Notebooks/google_news_headlines_by_date.csv'\n",
        "\n",
        "# Read the CSV and process it for sentiment analysis\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Assuming you want to analyze the sentiment of the first headline column as an example\n",
        "documents = df['headline_1'].dropna().tolist()  # Modify as needed for other columns\n",
        "\n",
        "sentiment_responses = analyze_sentiment(documents)\n",
        "\n",
        "for response in sentiment_responses:\n",
        "    if not response.is_error:\n",
        "        print(f\"Document: {response.sentences[0].text}\")\n",
        "        print(f\"Sentiment: {response.sentiment}, Scores: {response.confidence_scores}\")\n",
        "    else:\n",
        "        print(f\"Error: {response.error}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSnoo6kTn6BP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# !pip install flair\n",
        "\n",
        "# import flair\n",
        "# import scipy.linalg\n",
        "# print(scipy.linalg.triu)\n",
        "# from textblob import TextBlob\n",
        "# import os\n",
        "# import datetime\n",
        "# import numpy as np\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# print(\"Sentiment analysis imports.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK4JEMHmUXpP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import flair\n",
        "\n",
        "# flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "\n",
        "# negative_keywords = ['plunges', 'falls', 'down', 'losses', 'worst', 'declines', 'drops', 'suffers']\n",
        "\n",
        "# def simple_sentiment_heuristic(headline):\n",
        "#     \"\"\"\n",
        "#     Simple heuristic to detect negative sentiment based on specific keywords.\n",
        "#     \"\"\"\n",
        "#     for keyword in negative_keywords:\n",
        "#         if keyword in headline.lower():\n",
        "#             return \"NEGATIVE\", 1.0\n",
        "#     return None, None\n",
        "\n",
        "# def get_sentiment_label(sentence):\n",
        "#     \"\"\"\n",
        "#     Determine sentiment label based on the Flair model prediction.\n",
        "#     \"\"\"\n",
        "#     flair_sentiment.predict(sentence)\n",
        "#     sentiment_score = sentence.labels[0].score\n",
        "#     if sentence.labels[0].value == 'NEGATIVE':\n",
        "#         sentiment_score = 1 - sentiment_score\n",
        "#     if sentiment_score > 0.55:\n",
        "#         return \"POSITIVE\", sentiment_score\n",
        "#     elif sentiment_score < 0.45:\n",
        "#         return \"NEGATIVE\", sentiment_score\n",
        "#     else:\n",
        "#         return \"NEUTRAL\", sentiment_score\n",
        "\n",
        "# def get_sentiment_report(input_filename, output_filename):\n",
        "#     data_df = pd.read_csv(input_filename)\n",
        "#     results = []\n",
        "\n",
        "#     for index, row in data_df.iterrows():\n",
        "#         date = row['date']\n",
        "#         daily_sentiments = []\n",
        "#         sentiment_confidences = []\n",
        "\n",
        "#         for headline in row[1:].dropna():\n",
        "#             heuristic_label, heuristic_confidence = simple_sentiment_heuristic(headline)\n",
        "#             if heuristic_label:\n",
        "#                 sentiment_label = heuristic_label\n",
        "#                 sentiment_confidence = heuristic_confidence\n",
        "#             else:\n",
        "#                 sentence = flair.data.Sentence(headline)\n",
        "#                 sentiment_label, sentiment_confidence = get_sentiment_label(sentence)\n",
        "\n",
        "#             daily_sentiments.append(sentiment_label)\n",
        "#             sentiment_confidences.append(sentiment_confidence)\n",
        "\n",
        "#         overall_sentiment = \"NEUTRAL\"\n",
        "#         if daily_sentiments:\n",
        "#             positive_count = daily_sentiments.count(\"POSITIVE\")\n",
        "#             negative_count = daily_sentiments.count(\"NEGATIVE\")\n",
        "#             if positive_count > negative_count:\n",
        "#                 overall_sentiment = \"POSITIVE\"\n",
        "#             elif negative_count > positive_count:\n",
        "#                 overall_sentiment = \"NEGATIVE\"\n",
        "\n",
        "#         overall_confidence = sum(sentiment_confidences) / len(sentiment_confidences) if sentiment_confidences else 0\n",
        "\n",
        "#         results.append([date, overall_sentiment, overall_confidence] + daily_sentiments)\n",
        "\n",
        "#     max_headlines = max(len(r) - 3 for r in results)\n",
        "#     column_names = ['date', 'overall_sentiment', 'confidence'] + [f'headline_{i}_sentiment' for i in range(1, max_headlines + 1)]\n",
        "#     results_df = pd.DataFrame(results, columns=column_names)\n",
        "\n",
        "#     results_df.to_csv(output_filename, index=False)\n",
        "#     print(f\"Sentiment analysis results saved to {output_filename}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     input_filename = 'test_Sentiment_Data.csv'\n",
        "#     output_filename = 'sentiment_analysis_results_2.csv'\n",
        "#     get_sentiment_report(input_filename, output_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}